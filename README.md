# An automated classification method for single cell RNA-Seq data based on the information content of individual genes
### Preprint
Check out our preprint [here](https://www.biorxiv.org/content/biorxiv/early/2020/11/09/2020.11.02.365510.full.pdf)

### Abstract

Single cell RNA sequencing (scRNA-seq) technologies promise to enable the quantitative study of biological processes at the single cell level [Patel, A. P. et al. 2014; Treutlein, B. et al 2014; Miyamoto, D. T. et al. 2015]. Commercial platforms such as 10x chromium are becoming established in lab practice [Hwang, B. et al., 2018; Dong, M. B. et al., 2019; Xiong, X. et al. 2019]. More than other high-throughput technologies, however, the reproducibility and accuracy of current analysis pipelines remains challenging [Kiselev, V. Y. et al., 2019]. For example, cellular classification algorithms continue to be evaluated using datasets with cell labels generated by computational analysis of transcriptomic data validated by the manual application of a priori biologic knowledge [Pouyan M.B. et al, 2018; Zhang et al, 2018; Jiang, H. 2019]. Thus, there is a crucial need for a benchmark that provides ground truth labels in an independent manner. Here, we develop such a benchmark using a dataset where ground truth labels are generated from surface protein level measurements. We demonstrate a substantial decrease in estimated accuracy of the current gold-standard, Seurat algorithm [Satija R. et al 2015, Butler, A. et al. 2018], in data with low information content. In order to overcome the challenge posed by noisy uninformative data, we implement an algorithm that optimizes information content through an information theory-based approach. Our approach yields a dramatic improvement in accuracy for a couple of clustering algorithms.

### Prerequisites

Please install all required packages listed in python_requirement.txt files using either pip or conda

```
pip3 install -r requirements.txt
```
or

```
conda install -r requirements.txt
```

### Running the code

Please use the filtering.py file to run the workflow and submit it to HPC.

```
Python3 filtering.py
```

And repeat for another file (make sure you change the directory).

The output will be a CSV file with I as the information content for features.

There are two example output files listed in the folder.

## Authors 
* **Ziyou Ren** - *Bioinformatician* - [the University of Chicago](https://poiuy68.github.io/)
* **Martin Gerlach** -*Research Scientist* - [the Wikimedia Foundation](https://martingerlach.github.io/about/)
* **Hanyu Shi** -*Data Scientist*
* **GR Scott Budinger** -*Professor* - [Northwestern University](https://www.feinberg.northwestern.edu/faculty-profiles/az/profile.html?xid=10309)
* **LuiÃÅs A. Nunes Amaral** -*Professor* - [Northwestern University](https://amaral.northwestern.edu/)

See also the list of [contributors](https://github.com/poiuy68/TM_filtering_paper/commits) who participated in this project.

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* Northwestern University Quest and IT
* Driskill Graduate Program in Life Science
* American Heart Association

